{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Word Embedding\n",
    "Thực hiện chuyển đổi câu sang vector dùng Word2Vec. Tôi sẽ tạo một vector ngẫu nhiên cho từ UNK, từ UNK đại diện cho các từ hiếm gặp, không xuất hiện trong tập vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "def get_wv(w):\n",
    "    try:\n",
    "        return w2v.wv[w]\n",
    "    except KeyError:\n",
    "        return w2v.wv[\"UNK\"]\n",
    "\n",
    "df = pd.read_csv(\"./dataset/DanhgiaSmartphone.csv\")\n",
    "sentences = df[\"comment\"].values\n",
    "words = [[word for word in sen.lower().split()] for sen in sentences]\n",
    "words = [w + ['UNK'] for w in words]\n",
    "n = len(sentences)\n",
    "y = OneHotEncoder(sparse_output=False).fit_transform(df[\"label\"].values.reshape(-1, 1))\n",
    "# [001] pos\n",
    "# [100] neg\n",
    "# [010] neu\n",
    "# Chuyen word sang vector\n",
    "# w2v = Word2Vec(words, vector_size=10)\n",
    "# w2v.save(\"test\") \n",
    "w2v = Word2Vec.load(\"test\")\n",
    "\n",
    "X = []\n",
    "for word in words:\n",
    "    data = []\n",
    "    for w in word:\n",
    "        data.append(get_wv(w))\n",
    "    X.append(data)\n",
    "X = np.array(X, dtype=object)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Mô hình RNN\n",
    "\n",
    "Hình này đại diện cho những gì xảy ra ở 1 step\n",
    "<p align=\"center\" style=\"background: white\">\n",
    "<img src=\"https://images.viblo.asia/4b1cc09d-99fa-422a-9bee-14908aace750.png\" height=\"300\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "B1: Chúng ta có $h_{t-1}$ là state của các step phía trước. Bắt đầu, ta tính $z_t = x_t @ W_{hx} + b_h$ như thường lệ.\n",
    "Sau đó cộng $z_t$ với state của các step phía trước $z_t + (h_{t-1}@W_{hh})$. Tiếp tục đưa tất cả qua hàm kích hoạt $g_1$ để được state của step hiện tại $h_t = g_1(z_t + (h_{t-1}@W_{hh}))$.   \n",
    "B2: Nếu đầu vào chưa kết thúc, quay lại B1  \n",
    "B3: Đến đây, nếu đầu vào đã kết thúc (cuối câu) thì ta tính output $y_t = g_2(h_t @ W_{yh} + b_y)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import library, create train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from module.layers.Dense import Dense\n",
    "from module.optimizer.Adam import Adam\n",
    "from module.layers.RNN import RNN\n",
    "from tensorflow import keras\n",
    "from module.Sequential import Sequential\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"./dataset/DanhgiaSmartphone.csv\")\n",
    "data = df[[\"comment\", \"label\"]].values\n",
    "np.random.shuffle(data)\n",
    "print(len(data)) # 113\n",
    "train_data = data[:63]\n",
    "test_data = data[:63]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build word embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(113, 6, 10)\n",
      "(113, 3)\n"
     ]
    }
   ],
   "source": [
    "# Hàm lấy word embedding cho 1 từ\n",
    "def get_wv(w):\n",
    "    try:\n",
    "        return w2v.wv[w]\n",
    "    except KeyError:\n",
    "        return w2v.wv[\"UNK\"]\n",
    "\n",
    "# Hàm tạo word embedding cho 1 tập dữ liệu \n",
    "def word_embedding(sentences):\n",
    "    sen_split = [[word for word in sen.lower().split()]for sen in sentences]\n",
    "    x = []\n",
    "    for s in sen_split:\n",
    "        v = []\n",
    "        for w in s:\n",
    "            v.append(get_wv(w))\n",
    "        x.append(v)\n",
    "    x = keras.preprocessing.sequence.pad_sequences(x, padding=\"post\", dtype=\"float32\")\n",
    "    return x\n",
    "\n",
    "\"\"\" df = pd.read_csv(\"./dataset/UIT-ViSFD/Train.csv\")\n",
    "sen_embedding = df[\"comment\"]\n",
    "\n",
    "# Xóa dấu câu và số trong dataset dùng để xây dựng word embedding\n",
    "sen_embedding_clean = []\n",
    "for sen in sen_embedding:\n",
    "    clean_sen = re.sub(r'[^\\w\\s]', '', sen)\n",
    "    clean_sen = re.sub(r'\\d', '', clean_sen)\n",
    "    sen_embedding_clean.append(clean_sen)\n",
    "\n",
    "sen_embedding_clean = [[word for word in sen.lower().split()] for sen in sen_embedding_clean]\n",
    "# Chuyen word sang vector\n",
    "w2v = Word2Vec(sen_embedding_clean, vector_size=10)\n",
    "unk_vector = np.random.randn(10)\n",
    "w2v.wv.add_vector(\"UNK\", unk_vector)\n",
    "w2v.save(\"test.model\")  \"\"\"\n",
    "w2v = Word2Vec.load(\"test.model\")\n",
    "\n",
    "x_train = word_embedding(data[:, 0])\n",
    "y_train = OneHotEncoder(sparse_output=False).fit_transform(data[:, 1].reshape(-1, 1))\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0  [==========]  loss: 0.3623, accuracy 39.82%\n",
      "Epoch 1  [==========]  loss: 0.3471, accuracy 40.71%\n",
      "Epoch 2  [==========]  loss: 0.3303, accuracy 43.36%\n",
      "Epoch 3  [==========]  loss: 0.3076, accuracy 48.67%\n",
      "Epoch 4  [==========]  loss: 0.2914, accuracy 51.33%\n",
      "Epoch 5  [==========]  loss: 0.2749, accuracy 52.21%\n",
      "Epoch 6  [==========]  loss: 0.2613, accuracy 59.29%\n",
      "Epoch 7  [==========]  loss: 0.2535, accuracy 69.91%\n",
      "Epoch 8  [==========]  loss: 0.2361, accuracy 64.60%\n",
      "Epoch 9  [==========]  loss: 0.2217, accuracy 66.37%\n",
      "Epoch 10 [==========]  loss: 0.2054, accuracy 73.45%\n",
      "Epoch 11 [==========]  loss: 0.2005, accuracy 76.99%\n",
      "Epoch 12 [==========]  loss: 0.2089, accuracy 73.45%\n",
      "Epoch 13 [==========]  loss: 0.1793, accuracy 79.65%\n",
      "Epoch 14 [==========]  loss: 0.1789, accuracy 76.11%\n",
      "Epoch 15 [==========]  loss: 0.1670, accuracy 80.53%\n",
      "Epoch 16 [==========]  loss: 0.1900, accuracy 76.99%\n",
      "Epoch 17 [==========]  loss: 0.1509, accuracy 82.30%\n",
      "Epoch 18 [==========]  loss: 0.1479, accuracy 84.07%\n",
      "Epoch 19 [==========]  loss: 0.1243, accuracy 88.50%\n",
      "Epoch 20 [==========]  loss: 0.1162, accuracy 88.50%\n",
      "Epoch 21 [==========]  loss: 0.1390, accuracy 83.19%\n",
      "Epoch 22 [==========]  loss: 0.1531, accuracy 83.19%\n",
      "Epoch 23 [==========]  loss: 0.1574, accuracy 79.65%\n",
      "Epoch 24 [==========]  loss: 0.1235, accuracy 83.19%\n",
      "Epoch 25 [==========]  loss: 0.1286, accuracy 85.84%\n",
      "Epoch 26 [==========]  loss: 0.0869, accuracy 88.50%\n",
      "Epoch 27 [==========]  loss: 0.0725, accuracy 91.15%\n",
      "Epoch 28 [==========]  loss: 0.0642, accuracy 91.15%\n",
      "Epoch 29 [==========]  loss: 0.0845, accuracy 88.50%\n",
      "Epoch 30 [==========]  loss: 0.0653, accuracy 91.15%\n",
      "Epoch 31 [==========]  loss: 0.1097, accuracy 86.73%\n",
      "Epoch 32 [==========]  loss: 0.1030, accuracy 91.15%\n",
      "Epoch 33 [==========]  loss: 0.0702, accuracy 92.04%\n",
      "Epoch 34 [==========]  loss: 0.0539, accuracy 92.04%\n",
      "Epoch 35 [==========]  loss: 0.0469, accuracy 92.92%\n",
      "Epoch 36 [==========]  loss: 0.0424, accuracy 92.92%\n",
      "Epoch 37 [==========]  loss: 0.0396, accuracy 92.92%\n",
      "Epoch 38 [==========]  loss: 0.0363, accuracy 94.69%\n",
      "Epoch 39 [==========]  loss: 0.0352, accuracy 94.69%\n",
      "Epoch 40 [==========]  loss: 0.0340, accuracy 95.58%\n"
     ]
    }
   ],
   "source": [
    "md = Sequential()\n",
    "md.add(RNN(32, active=\"relu\"))\n",
    "md.add(Dense(3, active=\"softmax\"))\n",
    "md.compile(optimizer=Adam(lr=0.003, beta1=0.9, beta2=0.99999))\n",
    "md.fit(X=x_train, y=y_train, batch_size=6, epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test model's accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.97\n"
     ]
    }
   ],
   "source": [
    "x_test = word_embedding(test_data[:, 0])\n",
    "y_test = OneHotEncoder(sparse_output=False).fit_transform(test_data[:, 1].reshape(-1, 1))\n",
    "pre, score = md.evalute(x_test, y_test)\n",
    "print(f\"Test accuracy: {score:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
